{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Pixel acc  mIoU   mAP       seconds\n",
      "gradcam             0.62  0.41  0.72   1052.000000\n",
      "layercam            0.65  0.41  0.71    977.612902\n",
      "eigengradcam        0.63  0.38  0.70   1107.848377\n",
      "eigencam            0.53  0.34  0.66   1160.776685\n",
      "xgradcam            0.55  0.35  0.70   1094.452638\n",
      "gradcam++           0.58  0.36  0.69   1045.633355\n",
      "gradcam++           0.55  0.32  0.65   1054.720808\n",
      "scorecam            0.68  0.48  0.79   9361.606697\n",
      "ablationcam         0.64  0.46  0.65   9306.145710\n",
      "DDS+layerCAM        0.65  0.41  0.71  22914.000000\n",
      "DDS+scorecam        0.67  0.48  0.79  46800.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# display CSV file\n",
    "file_name = \"extension_res.csv\"\n",
    "df = pd.read_csv(file_name, index_col=0) \n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/ytjun.9676348/ipykernel_3789136/412010781.py:68: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), Image.NEAREST),\n",
      "/home/ytjun/.conda/envs/fvit/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /scratch-local/ytjun.9676348/openai-2025-01-29-16-57-39-031237\n",
      "creating model and diffusion...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy import *\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utilities.metrices import *\n",
    "from utilities import render\n",
    "from utilities.saver import Saver\n",
    "from utilities.iou import IoU\n",
    "from data.Imagenet import Imagenet_Segmentation\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "# needed for extension explanation methods\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# modified ViT model\n",
    "from baselines_mod.ViT.ViT_pytorch_cam import vit_base_patch16_224 as vitmodel\n",
    "\n",
    "# For creating segmentation masks and attack\n",
    "# from Extensions_Table1 import generate_new_explanation_method, reshape_transform\n",
    "from baselines_mod.ViT.Extensions_Table1 import generate_new_explanation_method, reshape_transform\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Data transsformations\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "test_img_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_lbl_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224), Image.NEAREST),\n",
    "])\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "ds = Imagenet_Segmentation('datasets/gtsegs_ijcv.mat',\n",
    "                           transform=test_img_trans, target_transform=test_lbl_trans)\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=1, drop_last=False)\n",
    "model = vitmodel(pretrained=True).cuda().eval()\n",
    "metric = IoU(2, ignore_index=-1)\n",
    "iterator = tqdm(dl)\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "# DDS method\n",
    "import sys\n",
    "sys.path.append(\"./guided_diffusion\")\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    # model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "\n",
    "def diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image and classifier training.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        learn_sigma=True,\n",
    "        diffusion_steps=1000,\n",
    "        noise_schedule=\"linear\",\n",
    "        timestep_respacing=\"250\",\n",
    "        use_kl=False,\n",
    "        predict_xstart=False,\n",
    "        rescale_timesteps=False,\n",
    "        rescale_learned_sigmas=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def classifier_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for classifier models.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        image_size=256,\n",
    "        classifier_use_fp16=False,\n",
    "        classifier_width=128,\n",
    "        classifier_depth=2,\n",
    "        classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "        classifier_use_scale_shift_norm=True,  # False\n",
    "        classifier_resblock_updown=True,  # False\n",
    "        classifier_pool=\"attention\",\n",
    "    )\n",
    "\n",
    "\n",
    "def model_and_diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        image_size=256,\n",
    "        num_channels=256,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=64,\n",
    "        attention_resolutions=\"32,16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=True,\n",
    "        use_fp16=True,\n",
    "        use_new_attention_order=False,\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=4,\n",
    "        use_ddim=False,\n",
    "        model_path=\"./guided_diffusion/models/256x256_diffusion_uncond.pt\",\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser\n",
    "\n",
    "args_diff = create_argparser().parse_args([])\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "d_model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args_diff, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "d_model.load_state_dict(\n",
    "    dist_util.load_state_dict(args_diff.model_path, map_location=\"cpu\")\n",
    ")\n",
    "d_model.to(dist_util.dev())\n",
    "if args_diff.use_fp16:\n",
    "    d_model.convert_to_fp16()\n",
    "d_model.eval()\n",
    "device = next(d_model.parameters()).device\n",
    "\n",
    "shape = (1, 3, 256, 256)\n",
    "steps =  1000\n",
    "start = 0.0001\n",
    "end = 0.02\n",
    "trial_num = 2\n",
    "\n",
    "def range_of_delta(beta_s, beta_e, steps):\n",
    "    def delta_value(beta):\n",
    "        return (beta/(1-beta))**(0.5)\n",
    "    return (delta_value(beta_s), delta_value(beta_e))\n",
    "\n",
    "def beta(t, steps, start, end):\n",
    "    return (t-1)/(steps-1)*(end-start)+start\n",
    "\n",
    "def add_noise(x, delta, opt_t, steps, start, end):\n",
    "    return np.sqrt(1-beta(opt_t, steps, start, end))*(x + th.randn_like(x) * delta)\n",
    "\n",
    "def get_opt_t(delta, start, end, steps):\n",
    "    return np.clip(int(np.around(1+(steps-1)/(end-start)*(1-1/(1+delta**2)-start))), 0, steps)\n",
    "\n",
    "# opt_t = get_opt_t(delta, start, end, steps)\n",
    "\n",
    "def denoise(img, opt_t, steps, start, end, delta, direct_pred=False):\n",
    "    # Extra line of code that again prevents mismatch because some data is on cpu and some on gpu\n",
    "    #img = img.to(device).float()\n",
    "\n",
    "    img_xt = add_noise(img, delta, opt_t, steps, start, end).unsqueeze(0).to(device)\n",
    "\n",
    "    indices = list(range(opt_t))[::-1]\n",
    "    from tqdm.auto import tqdm\n",
    "    indices = tqdm(indices)\n",
    "    img_iter = img_xt\n",
    "    for i in indices:\n",
    "        t = th.tensor([i]*shape[0], device=device)\n",
    "        # t = t.to(device)\n",
    "        with th.no_grad():\n",
    "            out = diffusion.p_sample(\n",
    "                d_model,\n",
    "                img_iter,\n",
    "                t,\n",
    "                clip_denoised=args_diff.clip_denoised,\n",
    "                denoised_fn=None,\n",
    "                cond_fn=None,\n",
    "                model_kwargs={},\n",
    "            )\n",
    "            img_iter = out['sample']\n",
    "            if direct_pred:\n",
    "                return out['pred_xstart']\n",
    "    # img_iter = ((img_iter + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    # img_iter = img_iter.permute(0, 2, 3, 1)\n",
    "    # img_iter = img_iter.contiguous()\n",
    "    return img_iter\n",
    "trans_to_256= transforms.Compose([\n",
    "   transforms.Resize((256, 256)),])\n",
    "trans_to_224= transforms.Compose([\n",
    "   transforms.Resize((224, 224)),])\n",
    "delta_range = range_of_delta(start, end, steps)\n",
    "\n",
    "\n",
    "def drop_lowest_max_fuse(arr, ratio=10):\n",
    "    arr_out = arr[0]\n",
    "    arr_out[arr_out<np.percentile(arr_out, ratio)]=0\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_new = arr[i]\n",
    "        arr_new[arr_new<np.percentile(arr_new, ratio)]=0\n",
    "        arr_out = np.maximum(arr_out,arr_new)\n",
    "    return arr_out\n",
    "def max_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = np.maximum(arr_out,arr[i])\n",
    "    return arr_out\n",
    "def mean_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = arr_out + arr[i]\n",
    "    return arr_out/len(arr)\n",
    "def normal(arr):\n",
    "    return (arr - arr.min())/(arr.max() - arr.min())\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "# functions needed\n",
    "def attack(image, model, noise_level,label_index=None,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
    "    import torchattacks\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    atk = torchattacks.PGD(model, eps=noise_level, alpha=noise_level/5, steps=10)\n",
    "    atk.set_normalization_used(mean, std)\n",
    "    labels = torch.FloatTensor([0]*1000)\n",
    "    if label_index == None:\n",
    "        # with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        label_index = logits.argmax()\n",
    "        # print(label_index)\n",
    "\n",
    "    labels[label_index] = 1\n",
    "    labels = labels.reshape(1, 1000)\n",
    "    adv_images = atk(image, labels.float())\n",
    "    return adv_images\n",
    "    \n",
    "\n",
    "def eval_batch(image, labels, evaluator, index, method, DDS=False):\n",
    "    evaluator.zero_grad()\n",
    "\n",
    "    image.requires_grad = True\n",
    "\n",
    "    image = image.requires_grad_()\n",
    "    # predictions = evaluator(image)\n",
    "\n",
    "\n",
    "    if DDS is True:\n",
    "        noise_level=7/255\n",
    "        opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "        res_lists = []\n",
    "        for trial in range(trial_num):\n",
    "            seed = 42 + trial\n",
    "            set_seed(seed)\n",
    "            image_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(image.cuda()).squeeze(0), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            image_denoise_normal = image_denoised + torch.randn_like(image_denoised) * noise_level\n",
    "            image_denoise_normal = torch.squeeze(image_denoise_normal)\n",
    "            image_denoise_normal = torch.clamp(image_denoise_normal, -1, 1)\n",
    "\n",
    "            Res = generate_new_explanation_method(evaluator, image.cuda(), method, None)\n",
    "            Res = torch.from_numpy(Res)\n",
    "            Res = Res.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            res_lists.append(Res)\n",
    "\n",
    "        Res = drop_lowest_max_fuse(res_lists)\n",
    "        Res = torch.tensor(Res).cuda()\n",
    "    else:\n",
    "        Res = generate_new_explanation_method(evaluator, image.cuda(), method, None)\n",
    "        Res = torch.from_numpy(Res)\n",
    "        Res = Res.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "    Res = (Res - Res.min()) / (Res.max() - Res.min())\n",
    "\n",
    "    ret = Res.mean()\n",
    "\n",
    "    Res_1 = Res.gt(ret).type(Res.type())\n",
    "    Res_0 = Res.le(ret).type(Res.type())\n",
    "\n",
    "    Res_1_AP = Res\n",
    "    Res_0_AP = 1-Res\n",
    "\n",
    "    Res_1[Res_1 != Res_1] = 0\n",
    "    Res_0[Res_0 != Res_0] = 0\n",
    "    Res_1_AP[Res_1_AP != Res_1_AP] = 0\n",
    "    Res_0_AP[Res_0_AP != Res_0_AP] = 0\n",
    "\n",
    "\n",
    "    # TEST\n",
    "    pred = Res.clamp(min=0.0) / Res.max()\n",
    "    pred = pred.view(-1).data.cpu().numpy()\n",
    "    target = labels.view(-1).data.cpu().numpy()\n",
    "\n",
    "    output = torch.cat((Res_0, Res_1), 1)\n",
    "    output_AP = torch.cat((Res_0_AP, Res_1_AP), 1)\n",
    "\n",
    "    # Evaluate Segmentation\n",
    "    batch_inter, batch_union, batch_pix_correct, batch_label = 0, 0, 0, 0\n",
    "    batch_ap = 0\n",
    "\n",
    "    # Segmentation results\n",
    "    pix_correct, labeled = batch_pix_accuracy(output[0].data.cpu(), labels[0])\n",
    "    inter, union = batch_intersection_union(output[0].data.cpu(), labels[0], 2)\n",
    "    batch_pix_correct += pix_correct\n",
    "    batch_label += labeled\n",
    "    batch_inter += inter\n",
    "    batch_union += union\n",
    "\n",
    "    ap = np.nan_to_num(get_ap_scores(output_AP, labels))\n",
    "    batch_ap += ap\n",
    "\n",
    "    return batch_pix_correct, batch_label, batch_inter, batch_union, batch_ap, pred, target\n",
    "\n",
    "\n",
    "# ONLY IMPORTANT FOR MAKING A NEW FILE\n",
    "\n",
    "# import csv\n",
    "\n",
    "# # File name\n",
    "# file_name = \"extension_res.csv\"\n",
    "\n",
    "# # Data to write\n",
    "# header = [\"Pixel acc\", \"mIoU\", \"mAP\", \"seconds\"]  # Column names\n",
    "# data = [[\"gradcam\", round(pixAcc, 2), round(mIoU, 2), round(mAp, 2), 1052]]  # Row with \"gradcam\" as the first value\n",
    "\n",
    "# # Write to CSV\n",
    "# with open(file_name, \"w\", newline=\"\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"\"] + header)  # Add header row with an empty cell for row labels\n",
    "#     writer.writerows(data)         # Add data row\n",
    "\n",
    "# print(f\"CSV file '{file_name}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.04it/s]\n",
      "100%|██████████| 24/24 [00:02<00:00, 11.01it/s]\n",
      "100%|██████████| 34/34 [00:02<00:00, 14.12it/s]\n",
      "100%|██████████| 24/24 [00:02<00:00, 11.72it/s]\n",
      "/scratch-local/ytjun.9676348/ipykernel_3789136/412010781.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Res = torch.tensor(Res).cuda()\n",
      "  0%|          | 1/4276 [00:19<23:06:16, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.13it/s]\n",
      "100%|██████████| 24/24 [00:01<00:00, 12.21it/s]\n",
      "100%|██████████| 34/34 [00:02<00:00, 14.11it/s]\n",
      "100%|██████████| 24/24 [00:01<00:00, 12.12it/s]\n",
      "/scratch-local/ytjun.9676348/ipykernel_3789136/412010781.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Res = torch.tensor(Res).cuda()\n",
      "  0%|          | 2/4276 [00:28<15:48:32, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.13it/s]\n",
      "100%|██████████| 24/24 [00:01<00:00, 12.28it/s]\n",
      "100%|██████████| 34/34 [00:02<00:00, 14.14it/s]\n",
      "100%|██████████| 24/24 [00:01<00:00, 12.23it/s]\n",
      "/scratch-local/ytjun.9676348/ipykernel_3789136/412010781.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Res = torch.tensor(Res).cuda()\n",
      "  0%|          | 3/4276 [00:37<13:27:26, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 14.14it/s]\n",
      "100%|██████████| 24/24 [00:01<00:00, 12.28it/s]\n",
      " 85%|████████▌ | 29/34 [00:02<00:00, 13.91it/s]\n",
      "  0%|          | 3/4276 [00:44<17:28:37, 14.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcuda()   \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# evaluate batch\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m correct, labeled, inter, union, ap, pred, target \u001b[38;5;241m=\u001b[39m \u001b[43meval_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMETHOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# calculate metrics\u001b[39;00m\n\u001b[1;32m     42\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n",
      "Cell \u001b[0;32mIn[4], line 307\u001b[0m, in \u001b[0;36meval_batch\u001b[0;34m(image, labels, evaluator, index, method, DDS)\u001b[0m\n\u001b[1;32m    304\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m trial\n\u001b[1;32m    305\u001b[0m set_seed(seed)\n\u001b[1;32m    306\u001b[0m image_denoised \u001b[38;5;241m=\u001b[39m trans_to_224(\n\u001b[0;32m--> 307\u001b[0m     \u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans_to_256\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    309\u001b[0m image_denoise_normal \u001b[38;5;241m=\u001b[39m image_denoised \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(image_denoised) \u001b[38;5;241m*\u001b[39m noise_level\n\u001b[1;32m    310\u001b[0m image_denoise_normal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(image_denoise_normal)\n",
      "Cell \u001b[0;32mIn[4], line 225\u001b[0m, in \u001b[0;36mdenoise\u001b[0;34m(img, opt_t, steps, start, end, delta, direct_pred)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# t = t.to(device)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 225\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_diff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     img_iter \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m direct_pred:\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/gaussian_diffusion.py:422\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_sample\u001b[39m(\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    397\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m ):\n\u001b[1;32m    405\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    Sample x_{t-1} from the model at the given timestep.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m             - 'pred_xstart': a prediction of x_0.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     noise \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[1;32m    431\u001b[0m     nonzero_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    432\u001b[0m         (t \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m([\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    433\u001b[0m     )  \u001b[38;5;66;03m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/respace.py:91\u001b[0m, in \u001b[0;36mSpacedDiffusion.p_mean_variance\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_mean_variance\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     90\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/gaussian_diffusion.py:260\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m B, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B,)\n\u001b[0;32m--> 260\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_var_type \u001b[38;5;129;01min\u001b[39;00m [ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED, ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED_RANGE]:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B, C \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/respace.py:128\u001b[0m, in \u001b[0;36m_WrappedModel.__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale_timesteps:\n\u001b[1;32m    127\u001b[0m     new_ts \u001b[38;5;241m=\u001b[39m new_ts\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1000.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_num_steps)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/fvit/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/unet.py:661\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, y)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_blocks:\n\u001b[1;32m    660\u001b[0m     h \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([h, hs\u001b[38;5;241m.\u001b[39mpop()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(h)\n",
      "File \u001b[0;32m~/.conda/envs/fvit/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/unet.py:77\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m     75\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, emb)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/fvit/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/unet.py:297\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/nn.py:137\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[1;32m    136\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(params)\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/nn.py:149\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[1;32m    147\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args[length:])\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 149\u001b[0m     output_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/unet.py:303\u001b[0m, in \u001b[0;36mAttentionBlock._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    301\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(b, c, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    302\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x))\n\u001b[0;32m--> 303\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(h)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x \u001b[38;5;241m+\u001b[39m h)\u001b[38;5;241m.\u001b[39mreshape(b, c, \u001b[38;5;241m*\u001b[39mspatial)\n",
      "File \u001b[0;32m~/.conda/envs/fvit/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/gpfs/home4/ytjun/FViT/guided_diffusion/unet.py:349\u001b[0m, in \u001b[0;36mQKVAttentionLegacy.forward\u001b[0;34m(self, qkv)\u001b[0m\n\u001b[1;32m    347\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mreshape(bs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, ch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, length)\u001b[38;5;241m.\u001b[39msplit(ch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    348\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(math\u001b[38;5;241m.\u001b[39msqrt(ch))\n\u001b[0;32m--> 349\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbct,bcs->bts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# More stable with f16 than dividing afterwards\u001b[39;00m\n\u001b[1;32m    352\u001b[0m weight \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msoftmax(weight\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(weight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    353\u001b[0m a \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbts,bcs->bct\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight, v)\n",
      "File \u001b[0;32m~/.conda/envs/fvit/lib/python3.8/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## CHANGE Method!\n",
    "##########################################################\n",
    "\n",
    "# A single method can be:\n",
    "# 20 min - 4 hours\n",
    "# Check extension_res for explcit time\n",
    "\n",
    "# METHOD = 'gradcam'\n",
    "METHOD = 'scorecam'\n",
    "# METHOD = 'gradcam++'\n",
    "# METHOD = 'ablationcam'\n",
    "# METHOD = 'xgradcam'\n",
    "# METHOD = 'eigencam'\n",
    "# METHOD = 'eigengradcam'\n",
    "# METHOD = 'layercam'\n",
    "\n",
    "DDS = True\n",
    "# DDS = False\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n",
    "import time\n",
    "# import os\n",
    "\n",
    "if DDS is False:\n",
    "    start = time.time()\n",
    "\n",
    "total_inter, total_union, total_pix_correct, total_label = np.int64(0), np.int64(0), np.int64(0), np.int64(0)\n",
    "total_ap, total_acc = [], []\n",
    "predictions, targets = [], []\n",
    "for batch_idx, (image, labels) in enumerate(iterator):\n",
    "     \n",
    "    #  attack\n",
    "    images = attack(image.cuda(), model, 7/255)\n",
    "    labels = labels.cuda()   \n",
    "\n",
    "    # evaluate batch\n",
    "    correct, labeled, inter, union, ap, pred, target = eval_batch(images, labels, model, batch_idx, METHOD, DDS)\n",
    "\n",
    "    # calculate metrics\n",
    "    predictions.append(pred)\n",
    "    targets.append(target)\n",
    "    total_pix_correct += correct.astype('int64')\n",
    "    total_label += labeled.astype('int64')\n",
    "    total_inter += inter.astype('int64')\n",
    "    total_union += union.astype('int64')\n",
    "    total_ap += [ap]\n",
    "    pixAcc = np.float64(1.0) * total_pix_correct / (np.spacing(1, dtype=np.float64) + total_label)\n",
    "    IoU = np.float64(1.0) * total_inter / (np.spacing(1, dtype=np.float64) + total_union)\n",
    "    mIoU = IoU.mean()\n",
    "    mAp = np.mean(total_ap)\n",
    "    # iterator.set_description('pixAcc: %.4f, mIoU: %.4f, mAP: %.4f' % (pixAcc, mIoU, mAp))\n",
    "    \n",
    "    # count how many iteration\n",
    "    print(batch_idx, end=' ')\n",
    "\n",
    "    # clear loading bars\n",
    "    os.system('clear')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Pixel-wise Accuracy: %.2f\\n\" % (pixAcc))\n",
    "print(\"Mean IoU over %d classes: %.2f\\n\" % (2, mIoU))\n",
    "print(\"Mean AP over %d classes: %.2f\\n\" % (2, mAp))\n",
    "\n",
    "# check time\n",
    "if DDS is False:\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\"Duration of this cell in seconds: \", duration)\n",
    "    new_row = [METHOD, round(pixAcc, 2), round(mIoU, 2), round(mAp, 2), duration]\n",
    "else:\n",
    "    # add new row to csv\n",
    "    new_row = [METHOD, round(pixAcc, 2), round(mIoU, 2), round(mAp, 2), 0]\n",
    "    \n",
    "with open(file_name, \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(new_row)  \n",
    "\n",
    "# display CSV file\n",
    "df = pd.read_csv(file_name, index_col=0) \n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
