{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:01.634398Z",
     "iopub.status.busy": "2025-01-31T10:23:01.634060Z",
     "iopub.status.idle": "2025-01-31T10:23:06.461576Z",
     "shell.execute_reply": "2025-01-31T10:23:06.460983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytjun/.conda/envs/fvit/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy import *\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utilities.metrices import *\n",
    "\n",
    "from utilities import render\n",
    "from utilities.saver import Saver\n",
    "from utilities.iou import IoU\n",
    "\n",
    "from data.Imagenet import Imagenet_Segmentation\n",
    "\n",
    "\n",
    "from baselines.ViT.ViT_explanation_generator import Baselines, LRP\n",
    "from baselines.ViT.ViT_new import vit_base_patch16_224\n",
    "from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
    "from baselines.ViT.ViT_orig_LRP import vit_base_patch16_224 as vit_orig_LRP\n",
    "\n",
    "from baselines.ViT.ViT_new import deit_base_patch16_224\n",
    "from baselines.ViT.ViT_LRP import deit_base_patch16_224 as deit_LRP\n",
    "from baselines.ViT.ViT_orig_LRP import deit_base_patch16_224 as deit_orig_LRP\n",
    "\n",
    "from baselines.ViT.ViT_new import swin_base_patch16_224\n",
    "from baselines.ViT.ViT_LRP import swin_base_patch16_224 as swin_LRP\n",
    "from baselines.ViT.ViT_orig_LRP import swin_base_patch16_224 as swin_orig_LRP\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:06.463748Z",
     "iopub.status.busy": "2025-01-31T10:23:06.463365Z",
     "iopub.status.idle": "2025-01-31T10:23:06.469927Z",
     "shell.execute_reply": "2025-01-31T10:23:06.469390Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set arguments, for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:06.471848Z",
     "iopub.status.busy": "2025-01-31T10:23:06.471627Z",
     "iopub.status.idle": "2025-01-31T10:23:13.168352Z",
     "shell.execute_reply": "2025-01-31T10:23:13.167746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/ytjun.9714013/ipykernel_1846190/3525652650.py:115: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((224, 224), Image.NEAREST),\n",
      "/home/ytjun/.conda/envs/fvit/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4276 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In class Args below, between the green # or Number Sign:\n",
    "\n",
    "# STEP 1\n",
    "Change self.model to one of the below:\n",
    "-   'ViT'\n",
    "-   'DeiT'\n",
    "-   'Swin'\n",
    "\n",
    "# STEP 2\n",
    "Change self.method to one of the below :\n",
    "\n",
    "- for \"VTA\" ->         self.method ='transformer_attribution'\n",
    "- for \"LRP\" ->                      'full_lrp'\n",
    "- for \"raw_attn\" ->                 'attn_last_layer'\n",
    "- for \"rollout\" ->                  'rollout'\n",
    "- for \"GradCAM\" ->                  'attn_gradcam'\n",
    "- for \"DDS+VTA\" ->                  'ours'\n",
    "\n",
    "# STEP 3\n",
    "Run All cells from section 3 to the end.\n",
    "After running the output will be visible at the end of the file or in the jobfile output\n",
    "\n",
    "\"\"\"\n",
    "##############################################################################################################################################\n",
    "\n",
    "# Args\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.arc = 'vgg'\n",
    "        self.train_dataset = 'imagenet'\n",
    "        self.model = 'ViT'\n",
    "        self.method = 'attn_last_layer'\n",
    "        self.att_strategy = 'PGD'\n",
    "        self.thr = 0.0\n",
    "        self.K = 1\n",
    "        self.save_img = False\n",
    "        self.no_ia = False\n",
    "        self.no_fx = False\n",
    "        self.no_fgx = False\n",
    "        self.no_m = False\n",
    "        self.no_reg = False\n",
    "        self.is_ablation = False\n",
    "        self.imagenet_seg_path = 'datasets/gtsegs_ijcv.mat'\n",
    "##############################################################################################################################################\n",
    "args = Args()\n",
    "\n",
    "args.checkname = args.method + '_' + args.arc\n",
    "\n",
    "alpha = 2\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "num_workers = 0\n",
    "batch_size = 1\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "# hyperparameters\n",
    "num_workers = 0\n",
    "batch_size = 1\n",
    "\n",
    "cls = ['airplane',\n",
    "       'bicycle',\n",
    "       'bird',\n",
    "       'boat',\n",
    "       'bottle',\n",
    "       'bus',\n",
    "       'car',\n",
    "       'cat',\n",
    "       'chair',\n",
    "       'cow',\n",
    "       'dining table',\n",
    "       'dog',\n",
    "       'horse',\n",
    "       'motobike',\n",
    "       'person',\n",
    "       'potted plant',\n",
    "       'sheep',\n",
    "       'sofa',\n",
    "       'train',\n",
    "       'tv'\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "# Define Saver\n",
    "saver = Saver(args)\n",
    "saver.results_dir = os.path.join(saver.experiment_dir, 'results')\n",
    "if not os.path.exists(saver.results_dir):\n",
    "    os.makedirs(saver.results_dir)\n",
    "if not os.path.exists(os.path.join(saver.results_dir, 'input')):\n",
    "    os.makedirs(os.path.join(saver.results_dir, 'input'))\n",
    "if not os.path.exists(os.path.join(saver.results_dir, 'explain')):\n",
    "    os.makedirs(os.path.join(saver.results_dir, 'explain'))\n",
    "\n",
    "args.exp_img_path = os.path.join(saver.results_dir, 'explain/img')\n",
    "if not os.path.exists(args.exp_img_path):\n",
    "    os.makedirs(args.exp_img_path)\n",
    "args.exp_np_path = os.path.join(saver.results_dir, 'explain/np')\n",
    "if not os.path.exists(args.exp_np_path):\n",
    "    os.makedirs(args.exp_np_path)\n",
    "\n",
    "# Data\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "test_img_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_lbl_trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224), Image.NEAREST),\n",
    "])\n",
    "\n",
    "ds = Imagenet_Segmentation(args.imagenet_seg_path,\n",
    "                           transform=test_img_trans, target_transform=test_lbl_trans)\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=1, drop_last=False)\n",
    "\n",
    "if args.model == 'ViT':\n",
    "    # ViT Model\n",
    "    model = vit_base_patch16_224(pretrained=True).cuda()\n",
    "    model.eval()\n",
    "    baselines = Baselines(model)\n",
    "\n",
    "    # LRP\n",
    "    model_LRP = vit_LRP(pretrained=True).cuda()\n",
    "    model_LRP.eval()\n",
    "    lrp = LRP(model_LRP)\n",
    "\n",
    "    # orig LRP\n",
    "    model_orig_LRP = vit_orig_LRP(pretrained=True).cuda()\n",
    "    model_orig_LRP.eval()\n",
    "    orig_lrp = LRP(model_orig_LRP)\n",
    "\n",
    "elif args.model == 'DeiT':\n",
    "    # DeiT Model\n",
    "    model = deit_base_patch16_224(pretrained=True).cuda()\n",
    "    model.eval()\n",
    "    baselines = Baselines(model)\n",
    "\n",
    "    # LRP\n",
    "    model_LRP = deit_LRP(pretrained=True).cuda()\n",
    "    model_LRP.eval()\n",
    "    lrp = LRP(model_LRP)\n",
    "\n",
    "    # orig LRP\n",
    "    model_orig_LRP = deit_orig_LRP(pretrained=True).cuda()\n",
    "    model_orig_LRP.eval()\n",
    "    orig_lrp = LRP(model_orig_LRP)\n",
    "    \n",
    "elif args.model == 'Swin':\n",
    "    # Swin Model\n",
    "    model = swin_base_patch16_224(pretrained=True).cuda()\n",
    "    model.eval()\n",
    "    baselines = Baselines(model)\n",
    "\n",
    "    # LRP\n",
    "    model_LRP = swin_LRP(pretrained=True).cuda()\n",
    "    model_LRP.eval()\n",
    "    lrp = LRP(model_LRP)\n",
    "\n",
    "    # orig LRP\n",
    "    model_orig_LRP = swin_orig_LRP(pretrained=True).cuda()\n",
    "    model_orig_LRP.eval()\n",
    "    orig_lrp = LRP(model_orig_LRP)\n",
    "    \n",
    "metric = IoU(2, ignore_index=-1)\n",
    "\n",
    "iterator = tqdm(dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:13.170534Z",
     "iopub.status.busy": "2025-01-31T10:23:13.170166Z",
     "iopub.status.idle": "2025-01-31T10:23:39.245781Z",
     "shell.execute_reply": "2025-01-31T10:23:39.245131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /scratch-local/ytjun.9714013/openai-2025-01-31-11-23-14-007753\n",
      "creating model and diffusion...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./guided_diffusion\")\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    # model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "\n",
    "def diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image and classifier training.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        learn_sigma=True,\n",
    "        diffusion_steps=1000,\n",
    "        noise_schedule=\"linear\",\n",
    "        timestep_respacing=\"250\",\n",
    "        use_kl=False,\n",
    "        predict_xstart=False,\n",
    "        rescale_timesteps=False,\n",
    "        rescale_learned_sigmas=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def classifier_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for classifier models.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        image_size=256,\n",
    "        classifier_use_fp16=False,\n",
    "        classifier_width=128,\n",
    "        classifier_depth=2,\n",
    "        classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "        classifier_use_scale_shift_norm=True,  # False\n",
    "        classifier_resblock_updown=True,  # False\n",
    "        classifier_pool=\"attention\",\n",
    "    )\n",
    "\n",
    "\n",
    "def model_and_diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        image_size=256,\n",
    "        num_channels=256,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=64,\n",
    "        attention_resolutions=\"32,16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=True,\n",
    "        use_fp16=True,\n",
    "        use_new_attention_order=False,\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=4,\n",
    "        use_ddim=False,\n",
    "        model_path=\"./guided_diffusion/models/256x256_diffusion_uncond.pt\",\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser\n",
    "\n",
    "args_diff = create_argparser().parse_args([])\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "d_model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args_diff, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "d_model.load_state_dict(\n",
    "    dist_util.load_state_dict(args_diff.model_path, map_location=\"cpu\")\n",
    ")\n",
    "d_model.to(dist_util.dev())\n",
    "if args_diff.use_fp16:\n",
    "    d_model.convert_to_fp16()\n",
    "d_model.eval()\n",
    "device = next(d_model.parameters()).device\n",
    "\n",
    "shape = (1, 3, 256, 256)\n",
    "steps =  1000\n",
    "start = 0.0001\n",
    "end = 0.02\n",
    "trial_num = 2\n",
    "\n",
    "def range_of_delta(beta_s, beta_e, steps):\n",
    "    def delta_value(beta):\n",
    "        return (beta/(1-beta))**(0.5)\n",
    "    return (delta_value(beta_s), delta_value(beta_e))\n",
    "\n",
    "def beta(t, steps, start, end):\n",
    "    return (t-1)/(steps-1)*(end-start)+start\n",
    "\n",
    "def add_noise(x, delta, opt_t, steps, start, end):\n",
    "    return np.sqrt(1-beta(opt_t, steps, start, end))*(x + th.randn_like(x) * delta)\n",
    "\n",
    "def get_opt_t(delta, start, end, steps):\n",
    "    return np.clip(int(np.around(1+(steps-1)/(end-start)*(1-1/(1+delta**2)-start))), 0, steps)\n",
    "\n",
    "# opt_t = get_opt_t(delta, start, end, steps)\n",
    "\n",
    "def denoise(img, opt_t, steps, start, end, delta, direct_pred=False):\n",
    "    # Extra line of code that again prevents mismatch because some data is on cpu and some on gpu\n",
    "    #img = img.to(device).float()\n",
    "\n",
    "    img_xt = add_noise(img, delta, opt_t, steps, start, end).unsqueeze(0).to(device)\n",
    "\n",
    "    indices = list(range(opt_t))[::-1]\n",
    "    from tqdm.auto import tqdm\n",
    "    indices = tqdm(indices)\n",
    "    img_iter = img_xt\n",
    "    for i in indices:\n",
    "        t = th.tensor([i]*shape[0], device=device)\n",
    "        # t = t.to(device)\n",
    "        with th.no_grad():\n",
    "            out = diffusion.p_sample(\n",
    "                d_model,\n",
    "                img_iter,\n",
    "                t,\n",
    "                clip_denoised=args_diff.clip_denoised,\n",
    "                denoised_fn=None,\n",
    "                cond_fn=None,\n",
    "                model_kwargs={},\n",
    "            )\n",
    "            img_iter = out['sample']\n",
    "            if direct_pred:\n",
    "                return out['pred_xstart']\n",
    "    # img_iter = ((img_iter + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    # img_iter = img_iter.permute(0, 2, 3, 1)\n",
    "    # img_iter = img_iter.contiguous()\n",
    "    return img_iter\n",
    "trans_to_256= transforms.Compose([\n",
    "   transforms.Resize((256, 256)),])\n",
    "trans_to_224= transforms.Compose([\n",
    "   transforms.Resize((224, 224)),])\n",
    "delta_range = range_of_delta(start, end, steps)\n",
    "\n",
    "\n",
    "def drop_lowest_max_fuse(arr, ratio=10):\n",
    "    arr_out = arr[0]\n",
    "    arr_out[arr_out<np.percentile(arr_out, ratio)]=0\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_new = arr[i]\n",
    "        arr_new[arr_new<np.percentile(arr_new, ratio)]=0\n",
    "        arr_out = np.maximum(arr_out,arr_new)\n",
    "    return arr_out\n",
    "def max_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = np.maximum(arr_out,arr[i])\n",
    "    return arr_out\n",
    "def mean_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = arr_out + arr[i]\n",
    "    return arr_out/len(arr)\n",
    "def normal(arr):\n",
    "    return (arr - arr.min())/(arr.max() - arr.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:39.248247Z",
     "iopub.status.busy": "2025-01-31T10:23:39.247739Z",
     "iopub.status.idle": "2025-01-31T10:23:39.253228Z",
     "shell.execute_reply": "2025-01-31T10:23:39.252563Z"
    }
   },
   "outputs": [],
   "source": [
    "noise_level=7/255\n",
    "def attack(image, model, noise_level,label_index=None,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
    "    import torchattacks\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    atk = torchattacks.PGD(model, eps=noise_level, alpha=noise_level/5, steps=10)\n",
    "    atk.set_normalization_used(mean, std)\n",
    "    labels = torch.FloatTensor([0]*1000)\n",
    "    if label_index == None:\n",
    "        # with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        label_index = logits.argmax()\n",
    "        # print(label_index)\n",
    "\n",
    "    labels[label_index] = 1\n",
    "    labels = labels.reshape(1, 1000)\n",
    "    adv_images = atk(image, labels.float())\n",
    "    return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:39.255241Z",
     "iopub.status.busy": "2025-01-31T10:23:39.254874Z",
     "iopub.status.idle": "2025-01-31T10:23:39.275181Z",
     "shell.execute_reply": "2025-01-31T10:23:39.274560Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_pred(output):\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    # pred[0, 0] = 282\n",
    "    # print('Pred cls : ' + str(pred))\n",
    "    T = pred.squeeze().cpu().numpy()\n",
    "    T = np.expand_dims(T, 0)\n",
    "    T = (T[:, np.newaxis] == np.arange(1000)) * 1.0\n",
    "    T = torch.from_numpy(T).type(torch.FloatTensor)\n",
    "    Tt = T.cuda()\n",
    "\n",
    "    return Tt\n",
    "\n",
    "\n",
    "def eval_batch(image, labels, evaluator, index):\n",
    "    evaluator.zero_grad()\n",
    "    # Save input image\n",
    "    if args.save_img:\n",
    "        img = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "        img = 255 * (img - img.min()) / (img.max() - img.min())\n",
    "        img = img.astype('uint8')\n",
    "        Image.fromarray(img, 'RGB').save(os.path.join(saver.results_dir, 'input/{}_input.png'.format(index)))\n",
    "        Image.fromarray((labels.repeat(3, 1, 1).permute(1, 2, 0).data.cpu().numpy() * 255).astype('uint8'), 'RGB').save(\n",
    "            os.path.join(saver.results_dir, 'input/{}_mask.png'.format(index)))\n",
    "\n",
    "    image.requires_grad = True\n",
    "\n",
    "    image = image.requires_grad_()\n",
    "    predictions = evaluator(image)\n",
    "    \n",
    "    # segmentation test for the rollout baseline\n",
    "    if args.method == 'rollout':\n",
    "        Res = baselines.generate_rollout(image.cuda(), start_layer=1).reshape(batch_size, 1, 14, 14)\n",
    "    \n",
    "    # segmentation test for the LRP baseline (this is full LRP, not partial)\n",
    "    elif args.method == 'full_lrp':\n",
    "        Res = orig_lrp.generate_LRP(image.cuda(), method=\"full\").reshape(batch_size, 1, 224, 224)\n",
    "    \n",
    "    # segmentation test for our method\n",
    "    elif args.method == 'transformer_attribution':\n",
    "        Res = lrp.generate_LRP(image.cuda(), start_layer=1, method=\"transformer_attribution\").reshape(batch_size, 1, 14, 14)\n",
    "    \n",
    "    # segmentation test for the partial LRP baseline (last attn layer)\n",
    "    elif args.method == 'lrp_last_layer':\n",
    "        Res = orig_lrp.generate_LRP(image.cuda(), method=\"last_layer\", is_ablation=args.is_ablation)\\\n",
    "            .reshape(batch_size, 1, 14, 14)\n",
    "    \n",
    "    # segmentation test for the raw attention baseline (last attn layer)\n",
    "    elif args.method == 'attn_last_layer':\n",
    "        Res = orig_lrp.generate_LRP(image.cuda(), method=\"last_layer_attn\", is_ablation=args.is_ablation)\\\n",
    "            .reshape(batch_size, 1, 14, 14)\n",
    "    \n",
    "    # segmentation test for the GradCam baseline (last attn layer)\n",
    "    elif args.method == 'attn_gradcam':\n",
    "        Res = baselines.generate_cam_attn(image.cuda()).reshape(batch_size, 1, 14, 14)\n",
    "    \n",
    "    elif args.method == 'ours':\n",
    "        opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "        res_lists = []\n",
    "        for trial in range(trial_num):\n",
    "            seed = 42 + trial\n",
    "            set_seed(seed)\n",
    "            image_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(image.cuda()).squeeze(0), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            image_denoise_normal = image_denoised + torch.randn_like(image_denoised) * noise_level\n",
    "            image_denoise_normal = torch.squeeze(image_denoise_normal)\n",
    "            image_denoise_normal = torch.clamp(image_denoise_normal, -1, 1)\n",
    "\n",
    "            res = lrp.generate_LRP(image_denoise_normal.cuda().unsqueeze(0), start_layer=1, method=\"transformer_attribution\").reshape(batch_size, 1, 14, 14)\n",
    "            res_lists.append(res.data.cpu().numpy())\n",
    "\n",
    "        Res = drop_lowest_max_fuse(res_lists)\n",
    "        Res = torch.tensor(Res).cuda()\n",
    "\n",
    "    if args.method != 'full_lrp':\n",
    "        # interpolate to full image size (224,224)\n",
    "        Res = torch.nn.functional.interpolate(Res, scale_factor=16, mode='bilinear').cuda()\n",
    "    \n",
    "    # threshold between FG and BG is the mean    \n",
    "    Res = (Res - Res.min()) / (Res.max() - Res.min())\n",
    "\n",
    "    ret = Res.mean()\n",
    "\n",
    "    Res_1 = Res.gt(ret).type(Res.type())\n",
    "    Res_0 = Res.le(ret).type(Res.type())\n",
    "\n",
    "    Res_1_AP = Res\n",
    "    Res_0_AP = 1-Res\n",
    "\n",
    "    Res_1[Res_1 != Res_1] = 0\n",
    "    Res_0[Res_0 != Res_0] = 0\n",
    "    Res_1_AP[Res_1_AP != Res_1_AP] = 0\n",
    "    Res_0_AP[Res_0_AP != Res_0_AP] = 0\n",
    "\n",
    "\n",
    "    # TEST\n",
    "    pred = Res.clamp(min=args.thr) / Res.max()\n",
    "    pred = pred.view(-1).data.cpu().numpy()\n",
    "    target = labels.view(-1).data.cpu().numpy()\n",
    "    # print(\"target\", target.shape)\n",
    "\n",
    "    output = torch.cat((Res_0, Res_1), 1)\n",
    "    output_AP = torch.cat((Res_0_AP, Res_1_AP), 1)\n",
    "\n",
    "    if args.save_img:\n",
    "        # Save predicted mask\n",
    "        mask = F.interpolate(Res_1, [64, 64], mode='bilinear')\n",
    "        mask = mask[0].squeeze().data.cpu().numpy()\n",
    "        # mask = Res_1[0].squeeze().data.cpu().numpy()\n",
    "        mask = 255 * mask\n",
    "        mask = mask.astype('uint8')\n",
    "        imageio.imsave(os.path.join(args.exp_img_path, 'mask_' + str(index) + '.jpg'), mask)\n",
    "\n",
    "        relevance = F.interpolate(Res, [64, 64], mode='bilinear')\n",
    "        relevance = relevance[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "        # relevance = Res[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "        hm = np.sum(relevance, axis=-1)\n",
    "        maps = (render.hm_to_rgb(hm, scaling=3, sigma=1, cmap='seismic') * 255).astype(np.uint8)\n",
    "        imageio.imsave(os.path.join(args.exp_img_path, 'heatmap_' + str(index) + '.jpg'), maps)\n",
    "\n",
    "    # Evaluate Segmentation\n",
    "    batch_inter, batch_union, batch_pix_correct, batch_label = 0, 0, 0, 0\n",
    "    batch_ap = 0\n",
    "\n",
    "    # Segmentation results\n",
    "    pix_correct, labeled = batch_pix_accuracy(output[0].data.cpu(), labels[0])\n",
    "    inter, union = batch_intersection_union(output[0].data.cpu(), labels[0], 2)\n",
    "    batch_pix_correct += pix_correct\n",
    "    batch_label += labeled\n",
    "    batch_inter += inter\n",
    "    batch_union += union\n",
    "    # print(\"output\", output.shape)\n",
    "    # print(\"ap labels\", labels.shape)\n",
    "\n",
    "    # Calculate average precision\n",
    "    ap = np.nan_to_num(get_ap_scores(output_AP, labels))\n",
    "    batch_ap += ap\n",
    "\n",
    "    return batch_pix_correct, batch_label, batch_inter, batch_union, batch_ap, pred, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:23:39.277198Z",
     "iopub.status.busy": "2025-01-31T10:23:39.276801Z",
     "iopub.status.idle": "2025-01-31T11:11:37.920584Z",
     "shell.execute_reply": "2025-01-31T11:11:37.919980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixAcc: 0.6433, mIoU: 0.4285, mAP: 0.7797: 100%|██████████| 4276/4276 [48:24<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "total_inter, total_union, total_pix_correct, total_label = np.int64(0), np.int64(0), np.int64(0), np.int64(0)\n",
    "total_ap, total_acc = [], []\n",
    "\n",
    "predictions, targets = [], []\n",
    "for batch_idx, (image, labels) in enumerate(iterator):\n",
    "    if args.method == \"blur\":\n",
    "        images = (image[0].cuda(), image[1].cuda())\n",
    "    else:\n",
    "        images = image.cuda()\n",
    "    labels = labels.cuda()   \n",
    "     \n",
    "    if args.att_strategy == 'PGD':\n",
    "        images = attack(images.cuda(), model, noise_level)\n",
    "    \n",
    "    # print(\"image\", image.shape)\n",
    "    # print(\"lables\", labels.shape)\n",
    "\n",
    "    correct, labeled, inter, union, ap, pred, target = eval_batch(images, labels, model, batch_idx)\n",
    "\n",
    "    predictions.append(pred)\n",
    "    targets.append(target)\n",
    "    total_pix_correct += correct.astype('int64')\n",
    "    total_label += labeled.astype('int64')\n",
    "    total_inter += inter.astype('int64')\n",
    "    total_union += union.astype('int64')\n",
    "    total_ap += [ap]\n",
    "    pixAcc = np.float64(1.0) * total_pix_correct / (np.spacing(1, dtype=np.float64) + total_label)\n",
    "    IoU = np.float64(1.0) * total_inter / (np.spacing(1, dtype=np.float64) + total_union)\n",
    "    mIoU = IoU.mean()\n",
    "    mAp = np.mean(total_ap)\n",
    "    iterator.set_description('pixAcc: %.4f, mIoU: %.4f, mAP: %.4f' % (pixAcc, mIoU, mAp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T11:11:37.923011Z",
     "iopub.status.busy": "2025-01-31T11:11:37.922618Z",
     "iopub.status.idle": "2025-01-31T11:11:37.930064Z",
     "shell.execute_reply": "2025-01-31T11:11:37.929548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel-wise Accuracy: 0.64\n",
      "\n",
      "Mean IoU over 2 classes: 0.43\n",
      "\n",
      "Mean AP over 2 classes: 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txtfile = os.path.join(saver.experiment_dir, 'result_mIoU_%.4f.txt' % mIoU)\n",
    "# txtfile = 'result_mIoU_%.4f.txt' % mIoU\n",
    "fh = open(txtfile, 'w')\n",
    "print(\"Pixel-wise Accuracy: %.2f\\n\" % (pixAcc))\n",
    "print(\"Mean IoU over %d classes: %.2f\\n\" % (2, mIoU))\n",
    "print(\"Mean AP over %d classes: %.2f\\n\" % (2, mAp))\n",
    "\n",
    "fh.write(\"Pixel-wise Accuracy: %.2f\\n\" % (pixAcc))\n",
    "fh.write(\"Mean IoU over %d classes: %.2f\\n\" % (2, mIoU))\n",
    "fh.write(\"Mean AP over %d classes: %.2f\\n\" % (2, mAp))\n",
    "fh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
