{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define functions needed for visualisation\n",
    "\n",
    "import torch\n",
    "from baselines.ViT.ViT_explanation_generator import Baselines\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from samples.CLS2IDX import CLS2IDX\n",
    "\n",
    "from baselines.ViT.ViT_LRP import vit_base_patch16_224\n",
    "from baselines.ViT.ViT_new import vit_base_patch16_224 as vit_for_cam\n",
    "from baselines.ViT.ViT_explanation_generator import LRP\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# create heatmap from mask on image\n",
    "def show_cam_on_image(img, mask, method='default', p=False):\n",
    "    if method=='full' and not p:\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_HOT)\n",
    "    else:   \n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "model = vit_for_cam(pretrained=True).to(device)\n",
    "baselines = Baselines(model)\n",
    "\n",
    "# initialize ViT pretrained\n",
    "model = vit_base_patch16_224(pretrained=True).to(device)\n",
    "model.eval()\n",
    "attribution_generator = LRP(model)\n",
    "\n",
    "name_map_baselines = {\n",
    "    \"VTA\":'transformer_attribution', \"FLRP\":\"full\", 'PFRP':\"last_layer\", \"raw_attn\":\"attn_last_layer\", 'rollout':\"rollout\", \"GradCAM\":\"attn_gradcam\"\n",
    "}\n",
    "\n",
    "def gen_grad_cam(baselines, index, image):\n",
    "    output = baselines.model(image.unsqueeze(0).to(device), register_hook=True)\n",
    "    if index == None:\n",
    "        index = np.argmax(output.cpu().data.numpy())\n",
    "    one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "    one_hot[0][index] = 1\n",
    "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "    one_hot = torch.sum(one_hot.to(device) * output)\n",
    "    baselines.model.zero_grad()\n",
    "    one_hot.backward(retain_graph=True)\n",
    "    grad = baselines.model.blocks[-1].attn.get_attn_gradients()\n",
    "    cam = baselines.model.blocks[-1].attn.get_attention_map()\n",
    "    cam = cam[0, :, 0, 1:].reshape(-1, 14, 14)\n",
    "    grad = grad[0, :, 0, 1:].reshape(-1, 14, 14)\n",
    "    grad = grad.mean(dim=[1, 2], keepdim=True)\n",
    "    cam = (cam * grad).mean(0).clamp(min=0)\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "    return cam\n",
    "\n",
    "def show_cam_on_image_helper(original_image, transformer_attribution, method='default', p=False):\n",
    "    # image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    \n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution, method, p)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "def generate_visualization(original_image, class_index=None, method =\"transformer_attribution\", p=False):\n",
    "    importance_maps = get_importance_maps(original_image, class_index, method)\n",
    "    return show_cam_on_image_helper(original_image, importance_maps, method, p)\n",
    "\n",
    "def get_importance_maps(original_image, class_index=None, method =\"transformer_attribution\"):\n",
    "    if method == \"attn_gradcam\":  \n",
    "        transformer_attribution = gen_grad_cam(baselines, class_index, original_image).detach()\n",
    "    else:\n",
    "        transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).to(device), method=method, index=class_index).detach()\n",
    "    \n",
    "    if method==\"full\":\n",
    "        transformer_attribution = transformer_attribution.reshape(1, 1, 224, 224)\n",
    "    else:\n",
    "        transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
    "        transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    transformer_attribution = transformer_attribution.reshape(224, 224).to(device).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "    \n",
    "    return transformer_attribution\n",
    "\n",
    "def print_top_classes(predictions, **kwargs):    \n",
    "    # Print Top-5 predictions\n",
    "    prob = torch.softmax(predictions, dim=1)\n",
    "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
    "    max_str_len = 0\n",
    "    class_names = []\n",
    "    for cls_idx in class_indices:\n",
    "        class_names.append(CLS2IDX[cls_idx])\n",
    "        if len(CLS2IDX[cls_idx]) > max_str_len:\n",
    "            max_str_len = len(CLS2IDX[cls_idx])\n",
    "    \n",
    "    print('Top 5 classes:')\n",
    "    for cls_idx in class_indices:\n",
    "        output_string = '\\t{} : {}'.format(cls_idx, CLS2IDX[cls_idx])\n",
    "        output_string += ' ' * (max_str_len - len(CLS2IDX[cls_idx])) + '\\t\\t'\n",
    "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
    "        print(output_string)\n",
    "\n",
    "############################################################################################################################################################\n",
    "\n",
    "# Load DDS methods\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./guided_diffusion\")\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    # model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "\n",
    "def diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image and classifier training.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        learn_sigma=True,\n",
    "        diffusion_steps=1000,\n",
    "        noise_schedule=\"linear\",\n",
    "        timestep_respacing=\"250\",\n",
    "        use_kl=False,\n",
    "        predict_xstart=False,\n",
    "        rescale_timesteps=False,\n",
    "        rescale_learned_sigmas=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def classifier_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for classifier models.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        image_size=256,\n",
    "        classifier_use_fp16=False,\n",
    "        classifier_width=128,\n",
    "        classifier_depth=2,\n",
    "        classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "        classifier_use_scale_shift_norm=True,  # False\n",
    "        classifier_resblock_updown=True,  # False\n",
    "        classifier_pool=\"attention\",\n",
    "    )\n",
    "\n",
    "\n",
    "def model_and_diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        image_size=256,\n",
    "        num_channels=256,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=64,\n",
    "        attention_resolutions=\"32,16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=True,\n",
    "        use_fp16=True,\n",
    "        use_new_attention_order=False,\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=4,\n",
    "        use_ddim=False,\n",
    "        model_path=\"./guided_diffusion/models/256x256_diffusion_uncond.pt\",\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser\n",
    "\n",
    "args = create_argparser().parse_args([])\n",
    "\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "d_model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "d_model.load_state_dict(\n",
    "    dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n",
    ")\n",
    "d_model.to(dist_util.dev())\n",
    "if args.use_fp16:\n",
    "    d_model.convert_to_fp16()\n",
    "d_model.eval()\n",
    "device = next(d_model.parameters()).device\n",
    "\n",
    "shape = (1, 3, 256, 256)\n",
    "steps=1000\n",
    "start=0.0001\n",
    "end=0.02\n",
    "\n",
    "def range_of_delta(beta_s, beta_e, steps):\n",
    "    def delta_value(beta):\n",
    "        return (beta/(1-beta))**(0.5)\n",
    "    return (delta_value(beta_s), delta_value(beta_e))\n",
    "\n",
    "def beta(t, steps, start, end):\n",
    "    return (t-1)/(steps-1)*(end-start)+start\n",
    "\n",
    "def add_noise(x, delta, opt_t, steps, start, end):\n",
    "    return np.sqrt(1-beta(opt_t, steps, start, end))*(x + th.randn_like(x) * delta)\n",
    "\n",
    "def get_opt_t(delta, start, end, steps):\n",
    "    return np.clip(int(np.around(1+(steps-1)/(end-start)*(1-1/(1+delta**2)-start))), 0, steps)\n",
    "\n",
    "# opt_t = get_opt_t(delta, start, end, steps)\n",
    "\n",
    "def denoise(img, opt_t, steps, start, end, delta, direct_pred=False):\n",
    "    # Extra line of code that again prevents mismatch because some data is on cpu and some on gpu\n",
    "    #img = img.to(device).float()\n",
    "\n",
    "    img_xt = add_noise(img, delta, opt_t, steps, start, end).unsqueeze(0).to(device)\n",
    "\n",
    "    indices = list(range(opt_t))[::-1]\n",
    "    from tqdm.auto import tqdm\n",
    "    indices = tqdm(indices)\n",
    "    img_iter = img_xt\n",
    "    for i in indices:\n",
    "        t = th.tensor([i]*shape[0], device=device)\n",
    "        # t = t.to(device)\n",
    "        with th.no_grad():\n",
    "            out = diffusion.p_sample(\n",
    "                d_model,\n",
    "                img_iter,\n",
    "                t,\n",
    "                clip_denoised=args.clip_denoised,\n",
    "                denoised_fn=None,\n",
    "                cond_fn=None,\n",
    "                model_kwargs={},\n",
    "            )\n",
    "            img_iter = out['sample']\n",
    "            if direct_pred:\n",
    "                return out['pred_xstart']\n",
    "    # img_iter = ((img_iter + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    # img_iter = img_iter.permute(0, 2, 3, 1)\n",
    "    # img_iter = img_iter.contiguous()\n",
    "    return img_iter\n",
    "trans_to_256= transforms.Compose([\n",
    "   transforms.Resize((256, 256)),])\n",
    "trans_to_224= transforms.Compose([\n",
    "   transforms.Resize((224, 224)),])\n",
    "delta_range = range_of_delta(start, end, steps)\n",
    "\n",
    "\n",
    "def drop_lowest_max_fuse(arr, ratio=10):\n",
    "    arr_out = arr[0]\n",
    "    arr_out[arr_out<np.percentile(arr_out, ratio)]=0\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_new = arr[i]\n",
    "        arr_new[arr_new<np.percentile(arr_new, ratio)]=0\n",
    "        arr_out = np.maximum(arr_out,arr_new)\n",
    "    return arr_out\n",
    "def max_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = np.maximum(arr_out,arr[i])\n",
    "    return arr_out\n",
    "def mean_fuse(arr):\n",
    "    arr_out = arr[0]\n",
    "    for i in range(1,len(arr)):\n",
    "        arr_out = arr_out + arr[i]\n",
    "    return arr_out/len(arr)\n",
    "def normal(arr):\n",
    "    return (arr - arr.min())/(arr.max() - arr.min())\n",
    "############################################################################################################################################################\n",
    "\n",
    "# Define PGD attack for image poisoning\n",
    "\n",
    "noise_level=7/255\n",
    "def attack(image, model, noise_level,label_index=None,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
    "    import torchattacks\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    atk = torchattacks.PGD(model, eps=noise_level, alpha=noise_level/5, steps=10)\n",
    "    atk.set_normalization_used(mean, std)\n",
    "    labels = torch.FloatTensor([0]*1000)\n",
    "    if label_index == None:\n",
    "        # with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        label_index = logits.argmax()\n",
    "        # print(label_index)\n",
    "\n",
    "    labels[label_index] = 1\n",
    "    labels = labels.reshape(1, 1000)\n",
    "    adv_images = atk(image, labels.float())\n",
    "    return adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reproduction of Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "gs = gridspec.GridSpec(2, 7, width_ratios=[1, 1, 1, 1, 1, 1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Choose image from samples\n",
    "path = \"./fig/catdog.png\"\n",
    "cls_idx = 243\n",
    "try: \n",
    "    image = Image.open(path)\n",
    "except:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "image_trans = transform(image)\n",
    "\n",
    "output = model(image_trans.unsqueeze(0).to(device))\n",
    "print_top_classes(output)\n",
    "\n",
    "# Place the original image in the middle left\n",
    "ax_input = plt.subplot(gs[:, 0])  # Span both rows for the left column\n",
    "ax_input.imshow(image)\n",
    "ax_input.axis('off')\n",
    "ax_input.set_title(\"Input\", fontsize=10, pad=30)\n",
    "ax_input.text(\n",
    "    0.5, 1.1, \"Dog: clean⟶\", ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "ax_input.text(\n",
    "    0.5, -0.15, \"Dog: poisoned⟶\", ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "ax_input.text(\n",
    "    0.5, -0.25, \"7/255\", fontsize=6, ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "\n",
    "# Remaining visualizations\n",
    "methods = ['last_layer_attn', 'rollout', 'attn_gradcam', 'full', None, 'transformer_attribution']\n",
    "titles = ['Raw Attention', 'Rollout', 'GradCam', 'LRP', 'VTA', 'Ours']\n",
    "\n",
    "# Generate poisoned image\n",
    "noise_level = 7 / 255\n",
    "image_trans_per_dog = attack(image_trans.unsqueeze(0).to(device), model, noise_level, label_index=cls_idx)\n",
    "\n",
    "for i, (method, title) in enumerate(zip(methods, titles)):\n",
    "    if title == 'Ours':\n",
    "        # Custom logic for \"Ours\"\n",
    "        steps = 1000\n",
    "        start = 0.0001\n",
    "        end = 0.02\n",
    "        trial_num = 2\n",
    "        opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "\n",
    "        dog_lists = []\n",
    "        dogp_lists = []\n",
    "        for _ in range(trial_num):\n",
    "            image_trans_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(image_trans), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            image_trans_denoise_normal = image_trans_denoised + torch.randn_like(image_trans_denoised) * noise_level\n",
    "            image_trans_denoise_normal = torch.squeeze(image_trans_denoise_normal)\n",
    "            image_trans_denoise_normal = torch.clamp(image_trans_denoise_normal, -1, 1)\n",
    "\n",
    "            poisoned_image = attack(image_trans.unsqueeze(0).to(device), model, noise_level, label_index=cls_idx)\n",
    "            poisoned_image_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(poisoned_image).squeeze(0), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            poisoned_image_denoised = poisoned_image_denoised + torch.randn_like(poisoned_image_denoised) * noise_level\n",
    "            poisoned_image_denoised = torch.squeeze(poisoned_image_denoised)\n",
    "            poisoned_image_denoised = torch.clamp(poisoned_image_denoised, -1, 1)\n",
    "\n",
    "            dog = get_importance_maps(image_trans_denoise_normal, class_index=cls_idx, method=\"transformer_attribution\")\n",
    "            dog_p = get_importance_maps(poisoned_image_denoised, class_index=cls_idx, method=\"transformer_attribution\")\n",
    "\n",
    "\n",
    "            dog_lists.append(dog)\n",
    "            dogp_lists.append(dog_p)\n",
    "\n",
    "        dog = drop_lowest_max_fuse(dog_lists)\n",
    "        dogp = drop_lowest_max_fuse(dogp_lists)\n",
    "        dog = normal(dog)\n",
    "        dogp = normal(dogp)\n",
    "\n",
    "        dog = show_cam_on_image_helper(image_trans_per_dog.squeeze(0), dog)\n",
    "        dog_p = show_cam_on_image_helper(image_trans_per_dog.squeeze(0), dogp)\n",
    "    else:\n",
    "        # Logic for other methods\n",
    "        dog = generate_visualization(image_trans, class_index=cls_idx, method=method) if method else generate_visualization(image_trans, class_index=cls_idx)\n",
    "        dog_p = generate_visualization(image_trans_per_dog.squeeze(0), class_index=cls_idx, method=method, p=True) if method else generate_visualization(image_trans_per_dog.squeeze(0), class_index=cls_idx, p=True)\n",
    "\n",
    "    # Top row\n",
    "    ax = plt.subplot(gs[0, i + 1])\n",
    "    ax.imshow(dog)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "    # Bottom row\n",
    "    ax_p = plt.subplot(gs[1, i + 1])\n",
    "    ax_p.imshow(dog_p)\n",
    "    ax_p.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(h_pad=0.8, w_pad=1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extensions visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "    methods_2 = the methpds\n",
    "    titles_2 = the plot title and is also responsible to activate DDS\n",
    "\n",
    "    dds_method = add new DDS+[method] for new combinations or else the DDS won't activate\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Change\n",
    "\n",
    "methods_2 = ['ablationcam', 'ablationcam', 'eigencam', 'eigencam', \"eigengradcam\", 'eigengradcam']\n",
    "titles_2 = ['AblationCAM', 'DDS+AblationCAM', 'EigenCAM', 'DDS+EigenCAM', 'EigenGradCAM', 'DDS+EigenGradCAM']\n",
    "dds_method = ['DDS+EigenCAM', 'DDS+EigenGradCAM', 'DDS+AblationCAM']\n",
    "\n",
    "\n",
    "# Keys are the methods activator\n",
    "\n",
    "# methods = \\\n",
    "#         {\"gradcam\": GradCAM,\n",
    "#          \"scorecam\": ScoreCAM,\n",
    "#          \"gradcam++\": GradCAMPlusPlus,\n",
    "#          \"ablationcam\": AblationCAM,\n",
    "#          \"xgradcam\": XGradCAM,\n",
    "#          \"eigencam\": EigenCAM,\n",
    "#          \"eigengradcam\": EigenGradCAM,\n",
    "#          \"layercam\": LayerCAM,\n",
    "#          \"fullgrad\": FullGrad}\n",
    "\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from baselines_mod.ViT.ViT_pytorch_cam import vit_base_patch16_224 as vitmodel\n",
    "\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "gs = gridspec.GridSpec(2, 5, width_ratios=[1, 1, 1, 1, 1] , height_ratios=[1, 1])\n",
    "\n",
    "# Choose image from samples\n",
    "path = \"./fig/catdog.png\"\n",
    "cls_idx = 243\n",
    "try: \n",
    "    image = Image.open(path)\n",
    "except:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "image_trans = transform(image)\n",
    "\n",
    "model = vitmodel(pretrained=True).to(device).eval()\n",
    "\n",
    "output = model(image_trans.unsqueeze(0).to(device))\n",
    "print_top_classes(output)\n",
    "\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_new_explanation_method(model, input_img, org_img, method=\"ablationcam\", targets=None):\n",
    "\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "    if method == 'ablationcam':\n",
    "        cam = methods[method](model=model,\n",
    "                target_layers=target_layers,\n",
    "                reshape_transform=reshape_transform,\n",
    "                ablation_layer=AblationLayerVit())\n",
    "    else:\n",
    "        cam = methods[method](model=model,\n",
    "                    target_layers=target_layers,\n",
    "                    reshape_transform=reshape_transform)\n",
    "\n",
    " \n",
    "    # targets = None\n",
    "    cam.batch_size = 32\n",
    "    grayscale_cam = cam(input_tensor=input_img,\n",
    "                    targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "\n",
    "    rgb_img = np.array(org_img)\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True, image_weight=0.5)\n",
    "    return cam_image\n",
    "\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "gs = gridspec.GridSpec(2, 7, width_ratios=[1, 1, 1, 1, 1, 1, 1], height_ratios=[1, 1])\n",
    "\n",
    "# Choose image from samples\n",
    "path = \"./fig/catdog.png\"\n",
    "cls_idx = 243\n",
    "try: \n",
    "    image = Image.open(path)\n",
    "except:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "image_trans = transform(image)\n",
    "\n",
    "model = vitmodel(pretrained=True).to(device).eval()\n",
    "\n",
    "output = model(image_trans.unsqueeze(0).to(device))\n",
    "print_top_classes(output)\n",
    "\n",
    "# Place the original image in the middle left\n",
    "ax_input = plt.subplot(gs[:, 0])  # Span both rows for the left column\n",
    "ax_input.imshow(image)\n",
    "ax_input.axis('off')\n",
    "ax_input.set_title(\"Input\", fontsize=10, pad=30)\n",
    "ax_input.text(\n",
    "    0.5, 1.1, \"Dog: clean⟶\", ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "ax_input.text(\n",
    "    0.5, -0.15, \"Dog: poisoned⟶\", ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "ax_input.text(\n",
    "    0.5, -0.25, \"7/255\", fontsize=6, ha='center', transform=ax_input.transAxes\n",
    ")\n",
    "\n",
    "# Remaining visualizations\n",
    "\n",
    "methods1 = {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate poisoned image\n",
    "noise_level = 8 / 255\n",
    "image_trans_per_dog = attack(image_trans.unsqueeze(0).to(device), model, noise_level, label_index=cls_idx)\n",
    "\n",
    "\n",
    "\n",
    "def generate_new_explanation_method_2(model, input_img, org_img, method=\"ablationcam\", targets=None):\n",
    "\n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "    target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "    if method == 'ablationcam':\n",
    "        cam = methods[method](model=model,\n",
    "                target_layers=target_layers,\n",
    "                reshape_transform=reshape_transform,\n",
    "                ablation_layer=AblationLayerVit())\n",
    "    else:\n",
    "        cam = methods[method](model=model,\n",
    "                    target_layers=target_layers,\n",
    "                    reshape_transform=reshape_transform)\n",
    "\n",
    " \n",
    "    # targets = None\n",
    "    cam.batch_size = 32\n",
    "    grayscale_cam = cam(input_tensor=input_img,\n",
    "                    targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "\n",
    "    rgb_img = np.array(org_img)\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True, image_weight=0.5)\n",
    "    return grayscale_cam\n",
    "\n",
    "for i, (method, title) in enumerate(zip(methods_2, titles_2)):\n",
    "   \n",
    "    # Logic for other methods\n",
    "\n",
    "    if title in dds_method:\n",
    "           # Custom logic for \"Ours\"\n",
    "        steps = 1000\n",
    "        start = 0.0001\n",
    "        end = 0.02\n",
    "        trial_num = 2\n",
    "        opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "\n",
    "        dog_lists = []\n",
    "        dogp_lists = []\n",
    "        for _ in range(trial_num):\n",
    "            image_trans_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(image_trans), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            image_trans_denoise_normal = image_trans_denoised + torch.randn_like(image_trans_denoised) * noise_level\n",
    "            image_trans_denoise_normal = torch.squeeze(image_trans_denoise_normal)\n",
    "            image_trans_denoise_normal = torch.clamp(image_trans_denoise_normal, -1, 1)\n",
    "\n",
    "            poisoned_image = attack(image_trans.unsqueeze(0).to(device), model, noise_level, label_index=cls_idx)\n",
    "            poisoned_image_denoised = trans_to_224(\n",
    "                denoise(trans_to_256(poisoned_image).squeeze(0), opt_t, steps, start, end, noise_level)\n",
    "            ).detach().cpu()\n",
    "            poisoned_image_denoised = poisoned_image_denoised + torch.randn_like(poisoned_image_denoised) * noise_level\n",
    "            poisoned_image_denoised = torch.squeeze(poisoned_image_denoised)\n",
    "            poisoned_image_denoised = torch.clamp(poisoned_image_denoised, -1, 1)\n",
    "\n",
    "\n",
    "            dog = generate_new_explanation_method_2(model, image_trans_denoise_normal.unsqueeze(0), image, method=method, targets=[ClassifierOutputTarget(243)])\n",
    "            dog_p = generate_new_explanation_method_2(model, poisoned_image_denoised.unsqueeze(0), image, method=method, targets=[ClassifierOutputTarget(243)])\n",
    "\n",
    "            dog_lists.append(dog)\n",
    "            dogp_lists.append(dog_p)\n",
    "\n",
    "\n",
    "        rgb_img = np.array(image)\n",
    "        rgb_img = np.float32(rgb_img) / 255\n",
    "\n",
    "        dog = drop_lowest_max_fuse(dog_lists)\n",
    "        dogp = drop_lowest_max_fuse(dogp_lists)\n",
    "        dog = normal(dog)\n",
    "        dogp = normal(dogp)\n",
    "\n",
    "        dog = show_cam_on_image(rgb_img, dog, use_rgb=True, image_weight=0.5)\n",
    "        dog_p = show_cam_on_image(rgb_img, dogp, use_rgb=True, image_weight=0.5)\n",
    "\n",
    "\n",
    "    else:\n",
    "        dog = generate_new_explanation_method(model, image_trans.unsqueeze(0), image, method=method, targets=[ClassifierOutputTarget(243)])\n",
    "        dog_p = generate_new_explanation_method(model, image_trans_per_dog, image, method=method, targets=[ClassifierOutputTarget(243)])\n",
    "\n",
    "    # Top row\n",
    "    ax = plt.subplot(gs[0, i + 1])\n",
    "    ax.imshow(dog)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=10)\n",
    "\n",
    "    # Bottom row\n",
    "    ax_p = plt.subplot(gs[1, i + 1])\n",
    "    ax_p.imshow(dog_p)\n",
    "    ax_p.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(h_pad=0.8, w_pad=1.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
